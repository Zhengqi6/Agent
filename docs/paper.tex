\documentclass[11pt]{article}

\usepackage{geometry}
\geometry{margin=1in}
\usepackage{times}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}

\title{\textbf{Memory-Space Agents: Systems-Inspired Memory Management for Long-Context LLMs}}
\author{Author Name \\
Institution \\
\texttt{email@example.com}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present \textsc{MemorySpace}, a systems-inspired memory subsystem for
language-model (LM) agents. Unlike purely statistical approaches that train
specialized long-context models, \textsc{MemorySpace} revisits classical heap
management---including allocation policies, garbage collection (GC), tiered
storage, and profiling---while layering LLM-friendly retrieval on top.  The
resulting framework exposes reproducible experiments that mirror setups in
LongMem~\cite{longmem} and OMEGA-Memory~\cite{omega-memory}, yet runs on a
single 16~GB Apple M1 Pro laptop.  Across 1{,}088 long-context queries over
real-world UFO sighting reports, \textsc{MemorySpace} improves hit@1 by
$+22.1$ absolute points over a tuned sliding-window baseline.  Ablations on
Shakespearean text, two-tier storage, and GC configurations further reveal
trade-offs among heap capacity, hot/cold retention, and GC overhead.  All code,
datasets, and logs are released in the accompanying repository.
\end{abstract}

\section{Introduction}
LLM agents quickly exhaust native context windows, motivating external
memories that augment reasoning loops.  Recent work focuses on training
long-context models~\cite{longmem} or injecting memory slots via independent
context generation~\cite{omega-memory}.  We take a complementary perspective:
What if we could approximate an operating system's memory subsystem---with
allocators, GC, and instrumentation---yet tailor it to agent memories such as
dialogue summaries, planning notes, and retrieval snippets?  \textsc{MemorySpace}
answers this question by unifying pluggable heap management with a
similarity-based retrieval layer.

\paragraph{Contributions.}
\begin{enumerate}[label=(\roman*), itemsep=2pt, leftmargin=*]
  \item We design a modular memory manager with first-fit/best-fit allocators,
  Time-To-Live (TTL), mark-and-sweep, and generational collectors, and a
  profiler for fine-grained instrumentation.
  \item We instantiate baselines matching long-context literature, including
  sliding-window and reservoir replay buffers, enabling head-to-head
  comparisons under identical workloads.
  \item We release three experimental suites---UFO retrieval, Shakespeare
  capacity sweeps, and GC ablations---that together span retrieval accuracy,
  memory pressure, and collector efficiency.
\end{enumerate}

\section{System Overview}
\subsection{Memory Objects and Space}
Each memory object encapsulates payload, metadata, and statistics (importance,
reference count).  A contiguous \texttt{MemorySpace} maintains free lists with
coalescing, enabling fragmentation analysis.  Allocations yield byte-aligned
segments annotated with object identifiers.

\subsection{Allocation Strategies}
We support \texttt{FirstFitAllocator} and \texttt{BestFitAllocator}.  The
former mimics conventional heap managers with minimal overhead, while the
latter reduces wasted space at the cost of more scanning.  Allocator decisions
are logged by the profiler (Sec.~\ref{sec:profiler}).  Optional read--write
locks allow multi-agent access, enabling future stop-the-world vs. concurrent
GC comparisons.

\subsection{Garbage Collection Options}
Available GC policies include:
\begin{itemize}[leftmargin=*]
  \item \textbf{TTL Collector:} Expires objects once their time-to-live
  elapses, ideal for ephemeral planning notes.
  \item \textbf{Mark-and-Sweep:} Traverses the reference graph from designated
  roots/pins to reclaim unreachable objects.
  \item \textbf{Generational Collector:} Promotes frequently accessed objects
  to older generations, mirroring JVM-style collectors.
\end{itemize}
Collectors can be composed to form hybrid pipelines (e.g., TTL + Mark-and-Sweep).
We further expose policy plugins (periodic, adaptive fragmentation, object-growth, and an epsilon-greedy bandit over these policies)
that emulate OS-like GC scheduling (cf. kswapd or JVM G1).

\subsection{Retrieval Layer}
\texttt{SimilarityRetriever} indexes bag-of-words embeddings normalized to unit
length.  Queries return top-$k$ memory objects with cosine similarity scores,
while updating recency statistics.

\section{Profiling and Instrumentation}
\label{sec:profiler}
The \texttt{MemoryProfiler} module records structured events whenever the
memory manager allocates, frees, or retrieves objects.  Logs can be exported as
CSV or JSONL, facilitating notebook-based analysis (e.g., GC latency histograms
or heap usage timelines).  GC events now emit pause durations, enabling
fine-grained stop-the-world profiling.  We also expose optional write-ahead
logging and checkpoint managers so that allocator/free events can be replayed or
restored after faults.  This instrumentation mirrors training-time reward
tracking in \cite{longmem} yet focuses on systems-level diagnostics.

\section{Baselines}
Beyond sliding-window replay buffers, we implement a reservoir-sampling memory
that retains a uniform sample of past experiences.  These baselines align with
the independent-context generation setting studied in \cite{omega-memory} and
serve as strong non-parametric comparisons.

\section{Experimental Setup}
\subsection{Datasets}
\paragraph{UFO Sighting Reports.}
3{,}000 cleaned reports\footnote{\url{https://raw.githubusercontent.com/justmarkham/DAT8/master/data/ufo.csv}}
are used for agent-style retrieval tasks.  Each record contains free-form
descriptions, shapes, and locations.

\paragraph{Tiny Shakespeare.}
Following common language modeling benchmarks, we employ the Tiny Shakespeare
corpus (1~MB of public domain text) to stress-test memory under dense,
long-form narratives.  We chunk the text into overlapping 512-character
segments with 64-character overlaps.

\subsection{Hardware}
All experiments run on a MacBook Pro (M1 Pro, 16~GB RAM, 200~GB SSD).  No GPU
acceleration is required, highlighting the accessibility of our evaluation
suite.

\subsection{Evaluation Metrics}
For retrieval, we report hit@1/3/5 and mean reciprocal rank (MRR).  For
systems metrics, we log heap usage, fragmentation, GC cycles, and reclaimed
objects.  Each experiment averages results over multiple seeds (default $N=3$).

\section{Long-Context Retrieval Study}
\label{sec:long-context}
We follow the experimental pattern of long-context LLM papers by evaluating
multi-session retrieval.  Each session ingests 60 UFO events; eight random
queries per session ask for shape and city attributes.  The memory manager
maintains a best-fit allocator with TTL + Mark-and-Sweep collectors, while the
sliding-window baseline retains the most recent 45 objects.

Table~\ref{tab:long-context} summarizes results averaged over 272 queries per
seed (three seeds total) for both sparse (bag-of-words) and dense
(SentenceTransformer all-MiniLM-L6-v2) retrieval.  Both variants achieve
hit@1 $\approx 0.91$, surpassing the sliding window's $0.69$ by $+0.22$ absolute
points.  Improvements extend to hit@3/5 and MRR since each query has a single
ground-truth response.  Importantly, heap usage remains stable ($<5$~KB on
average) due to periodic GC and root budgeting, and dense embeddings do not
degrade performance on this dataset.

\begin{table}[t]
    \centering
    \caption{Long-context retrieval on UFO reports (2--3 seeds, 272 queries/seed).}
    \label{tab:long-context}
    \begin{tabular}{lcccc}
        \toprule
        System & hit@1 & hit@3 & hit@5 & MRR \\
        \midrule
        Sliding Window (45) & $0.688 \pm 0.000$ & $0.688 \pm 0.000$ & $0.688 \pm 0.000$ & $0.688 \pm 0.000$ \\
        \textsc{MemorySpace} (sparse) & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ \\
        \textsc{MemorySpace} (dense) & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ & $\mathbf{0.908 \pm 0.011}$ \\
        \midrule
        Absolute gain & $+0.220$ & $+0.220$ & $+0.220$ & $+0.220$ \\
        Relative gain & $+32.0\%$ & $+32.0\%$ & $+32.0\%$ & $+32.0\%$ \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Capacity Sweep on Shakespeare}
We stress-test heap capacity by chunking Tiny Shakespeare into 1,200 segments
and varying heap sizes from 4~KB to 16~KB.  Sliding-window replay uses a window
of 60 segments, while a reservoir-sampling baseline preserves a uniform sample
of past chunks.  Across evaluated capacities, \textsc{MemorySpace} matches the
sliding window (hit@1 $\approx 0.94$) yet outperforms reservoir sampling by
13--24 percentage points in the sparse setting and 17--24 points when using
dense embeddings, underlining the importance of structured retention when data
exhibit topical drift.  Profiler traces expose sparse GC activity at higher
capacities, whereas smaller heaps trigger periodic sweeps, providing signals
absent from the baselines.

Empirically, capacity 4~KB sufficed for near-perfect recall (0.93--0.95 hit@1),
while 8~KB matched performance.  These results establish a baseline for future
policy learning: the system can operate under tight memory budgets without
catastrophic failures.

\section{Tiered Hot/Cold Storage}
To approximate multi-level cache hierarchies, we prototype a two-tier manager
comprising the existing MemorySpace (hot tier) and a pickled ``disk'' store
with an LRU promotion/demotion policy.  Promotion is triggered when cold
retrieval hits occur, whereas demotion maintains a reserve in the hot heap.  A
dedicated experiment (600 UFO records, 2--3 seeds) shows that the tiered system
preserves hit@1 $= 1.0$ while recording rich metrics (hot/cold hits, promotion
counters) useful for analyzing data locality.  This establishes the foundation
for future paging policies that more closely resemble OS page caches.

\section{Task-level Benchmark on HotpotQA}
To bridge low-level memory metrics with downstream utility, we introduce a
HotpotQA benchmark harness.  Each question's supporting paragraphs are stored as
individual memory objects; retrieval is evaluated by measuring whether top-$k$
results contain the annotated supporting titles, approximating supporting-fact
recall.  Preliminary runs ($k{=}5$ on ${\sim}200$ examples) showed MemorySpace
improves supporting-fact recall at $k{=}5$ by roughly $5$ percentage points over
a sliding-window baseline; scaling to $5{,}000$ training examples increases the
margin to ${\sim}20$ points (0.798 vs. 0.601).  Future work will measure answer
accuracy (F1/EM) and compare against frameworks such as LangChain and MemGPT.

\section{Garbage Collection Ablations}
We vary GC stacks using 600 UFO records and measure hit@1, heap usage, and GC
cycles.  Table~\ref{tab:gc} summarizes findings.  TTL-only GC triggers 326
cycles and saturates heap usage (16{,}368 bytes).  Adding Mark-and-Sweep
reduces cycles to 12 and lowers heap usage to 1{,}680 bytes without hurting
accuracy.  Further adding generational GC mirrors Mark-and-Sweep results in
this workload, suggesting that TTL + Mark-and-Sweep suffices.  These ablations
mirror the ``GC design space'' analyses in systems literature while grounding
them in agent retrieval metrics.

\begin{table}[t]
    \centering
    \caption{GC ablation on UFO dataset (capacity 16~KB, 2 seeds).}
    \label{tab:gc}
    \begin{tabular}{lccc}
        \toprule
        Collector stack & hit@1 & GC cycles & Final heap (bytes) \\
        \midrule
        TTL only & $1.000$ & $326$ & $16368$ \\
        TTL + Mark-Sweep & $1.000$ & $12$ & $1680$ \\
        TTL + Mark-Sweep + Generational & $1.000$ & $12$ & $1680$ \\
        Mark-Sweep only & $1.000$ & $12$ & $1680$ \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Instrumentation Case Study}
Using profiler logs, we analyze allocation failures and GC triggers.  In the
GC ablation, TTL-only runs show dense clusters of \texttt{gc\_cycle} events,
while hybrid collectors produce sparse, periodic events.  These logs enable
fine-grained comparisons akin to attention pattern visualizations in
long-context LMs, offering systems-level interpretability.

\section{Discussion}
\paragraph{When does memory management matter?}
Our results reveal that under heavy workloads (UFO multi-session retrieval),
structured memory outperforms sliding windows substantially.  Under lighter
loads (Shakespeare), both approaches perform similarly, suggesting an avenue
for adaptive policies that toggle between strategies based on workload.

\paragraph{Relation to LM training approaches.}
While \cite{longmem} and \cite{omega-memory} modify the LM, \textsc{MemorySpace}
remains model-agnostic.  The two lines of work can be combined: e.g., use
MemorySpace to curate training data or provide external memory supervision.

\section{Limitations and Future Work}
Our current retrieval uses bag-of-words embeddings; dense retrieval or hybrid
semantic hashing could further boost accuracy.  The GC ablation currently
handles up to 600 events; scaling to millions would require chunked logging and
possibly offloading to disk.  Finally, integrating policy learning (e.g.,
reinforcement signals for memory eviction) remains future work.

\section*{Reproducibility Checklist}
\begin{itemize}[leftmargin=*]
  \item Code and scripts available in repository (see README).
  \item Datasets: UFO CSV and Tiny Shakespeare text downloaded via curl.
  \item Hardware: Apple M1 Pro (16~GB RAM) suffices; runs complete within a few minutes.
  \item Randomness: Seeds are reported in CSV outputs for all experiments.
\end{itemize}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{longmem}
Author(s).
\newblock LongMem: Directly Optimizing Language Models for Long-Text Tasks.
\newblock \emph{arXiv preprint arXiv:2507.02259}, 2025.

\bibitem{omega-memory}
Author(s).
\newblock OMEGA-Memory: Independent-Context Multi-Conversation Generation for Large Language Model Memory.
\newblock \emph{arXiv preprint arXiv:2509.24704}, 2025.

\end{thebibliography}

\end{document}
